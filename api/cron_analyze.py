import os
import json
import time
from typing import List, Dict, Any, Tuple

from flask import Flask, jsonify, request
import requests

try:
    from google.cloud import firestore
    from google.oauth2 import service_account
except Exception:
    firestore = None
    service_account = None

try:
    from youtube_transcript_api import (
        YouTubeTranscriptApi,
        TranscriptsDisabled,
        NoTranscriptFound,
        VideoUnavailable,
    )
except Exception:
    YouTubeTranscriptApi = None


app = Flask(__name__)


@app.after_request
def add_cors_headers(resp):
    try:
        resp.headers['Access-Control-Allow-Origin'] = '*'
        resp.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
        resp.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
    except Exception:
        pass
    return resp


def _load_db():
    if firestore is None or service_account is None:
        raise RuntimeError('Firestore libraries not available')
    creds_json = os.getenv('GOOGLE_CLOUD_CREDENTIALS') or os.getenv('GOOGLE_APPLICATION_CREDENTIALS_JSON')
    if not creds_json:
        raise RuntimeError('Missing GOOGLE_CLOUD_CREDENTIALS env with service account JSON')
    info = json.loads(creds_json)
    credentials = service_account.Credentials.from_service_account_info(info)
    project_id = info.get('project_id')
    return firestore.Client(project=project_id, credentials=credentials)


def _call_gemini(system_prompt: str, user_content: str) -> str:
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise RuntimeError('GEMINI_API_KEY not set')
    model = 'models/gemini-1.5-pro-latest'
    url = f'https://generativelanguage.googleapis.com/v1beta/{model}:generateContent?key={api_key}'
    payload = {
        'contents': [
            { 'role': 'user', 'parts': [{ 'text': f"{system_prompt}\n\n{user_content}" }] }
        ],
        'generationConfig': { 'temperature': 0.3 }
    }
    res = requests.post(url, json=payload, timeout=90)
    res.raise_for_status()
    data = res.json()
    return data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')


def _build_category_prompt() -> str:
    return (
        'ë‹¤ìŒ ëŒ€ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ì•„ëž˜ í˜•ì‹ìœ¼ë¡œë§Œ í•œ ì¤„ì”© ì •í™•ížˆ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸/ë¨¸ë¦¬ë§/ì„¤ëª… ê¸ˆì§€.\n'
        'í•œêµ­ ëŒ€ ì¹´í…Œê³ ë¦¬: \ní•œêµ­ ì¤‘ ì¹´í…Œê³ ë¦¬: \ní•œêµ­ ì†Œ ì¹´í…Œê³ ë¦¬: \n'
        'EN Main Category: \nEN Sub Category: \nEN Micro Topic: \n'
        'ì¤‘êµ­ ëŒ€ ì¹´í…Œê³ ë¦¬: \nì¤‘êµ­ ì¤‘ ì¹´í…Œê³ ë¦¬: \nì¤‘êµ­ ì†Œ ì¹´í…Œê³ ë¦¬: '
    )


def _build_keywords_prompt() -> str:
    return (
        'ì•„ëž˜ ì œê³µëœ "ì œëª©"ê³¼ "ëŒ€ë³¸"ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, ì›ë³¸ ì˜ìƒì„ ê²€ìƒ‰í•´ ì°¾ê¸° ì‰¬ìš´ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´/ì˜ì–´/ì¤‘êµ­ì–´ë¡œ ê°ê° 8~15ê°œì”© ì¶”ì¶œí•˜ì„¸ìš”.\n'
        'ì¶œë ¥ í˜•ì‹ì€ JSON ê°ì²´ë§Œ, ë‹¤ë¥¸ ì„¤ëª…/ë¨¸ë¦¬ë§/ì½”ë“œíŽœìŠ¤ ê¸ˆì§€.\n'
        'ìš”êµ¬ í˜•ì‹: {"ko":["í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2",...],"en":["keyword1",...],"zh":["å…³é”®è¯1",...]}\n'
        'ê·œì¹™:\n- ê° í‚¤ì›Œë“œëŠ” 1~4ë‹¨ì–´ì˜ ì§§ì€ êµ¬ë¡œ ìž‘ì„±\n- í•´ì‹œíƒœê·¸/íŠ¹ìˆ˜ë¬¸ìž/ë”°ì˜´í‘œ ì œê±°, ë¶ˆìš©ì–´ ì œì™¸\n- ë™ì¼ ì˜ë¯¸/ì¤‘ë³µ í‘œí˜„ì€ í•˜ë‚˜ë§Œ ìœ ì§€\n- ì¸ëª…/ì±„ë„ëª…/ë¸Œëžœë“œ/í•µì‹¬ ì£¼ì œ í¬í•¨\n'
    )


def _build_material_prompt() -> str:
    return 'ë‹¤ìŒ ëŒ€ë³¸ì˜ í•µì‹¬ ì†Œìž¬ë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ì¤„ë¡œë§Œ, "ì†Œìž¬: "ë¡œ ì‹œìž‘í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ìžëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.'


def _build_dopamine_prompt(sentences: List[str]) -> str:
    header = 'ë‹¤ìŒ "ë¬¸ìž¥ ë°°ì—´"ì— ëŒ€í•´, ê° ë¬¸ìž¥ë³„ë¡œ ê¶ê¸ˆì¦/ë„íŒŒë¯¼ ìœ ë°œ ì •ë„ë¥¼ 1~10 ì •ìˆ˜ë¡œ í‰ê°€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨ížˆ ì„¤ëª…í•˜ì„¸ìš”. ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œë§Œ, ìš”ì†ŒëŠ” {"sentence":"ë¬¸ìž¥","level":ì •ìˆ˜,"reason":"ì´ìœ "} í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”. ì—¬ëŠ” ëŒ€ê´„í˜¸ë¶€í„° ë‹«ëŠ” ëŒ€ê´„í˜¸ê¹Œì§€ ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.'
    return header + '\n\në¬¸ìž¥ ë°°ì—´:\n' + json.dumps(sentences, ensure_ascii=False)


def _build_analysis_prompt() -> str:
    # ì¶•ì•½ ì—†ì´ ë™ì¼ í…œí”Œë¦¿ ìœ ì§€
    return (
        "[GPTs Instructions ìµœì¢…ì•ˆ]\n\níŽ˜ë¥´ì†Œë‚˜ (Persona)\n\në‹¹ì‹ ì€ \"ëŒ€ë³¸ë¶„ì„_ë£°ë£¨ëž„ë¼ë¦´ë¦¬\"ìž…ë‹ˆë‹¤. ìœ íŠœë¸Œ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ ì½˜í…ì¸  ì „ëžµ ìˆ˜ë¦½ê³¼ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ë•ëŠ” ìµœê³ ì˜ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ í•­ìƒ ì²´ê³„ì ì´ê³ , ê¹”ë”í•˜ë©°, ì‚¬ìš©ìžê°€ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìžˆë„ë¡ ì™„ë²½í•˜ê²Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n\ní•µì‹¬ ìž„ë¬´ (Core Mission)\n\nì‚¬ìš©ìžê°€ ìœ íŠœë¸Œ ëŒ€ë³¸(ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´)ì„ ìž…ë ¥í•˜ë©´, ì•„ëž˜ 4ë²ˆ í•­ëª©ì˜ **[ì¶œë ¥ í…œí”Œë¦¿]**ì„ ë‹¨ í•˜ë‚˜ì˜ ê¸€ìžë‚˜ ê¸°í˜¸ë„ í‹€ë¦¬ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n\nì ˆëŒ€ ê·œì¹™ (Golden Rules)\n\nê·œì¹™ 1: í…œí”Œë¦¿ ë³µì œ - ì¶œë ¥ë¬¼ì˜ êµ¬ì¡°, ë””ìžì¸, ìˆœì„œ, í•­ëª© ë²ˆí˜¸, ì´ëª¨ì§€(âœ¨, ðŸ“Œ, ðŸŽ¬, ðŸ§, ðŸ’¡, âœ…, ðŸ¤”), ê°•ì¡°(), êµ¬ë¶„ì„ (*) ë“± ëª¨ë“  ì‹œê°ì  ìš”ì†Œë¥¼ ì•„ëž˜ **[ì¶œë ¥ í…œí”Œë¦¿]**ê³¼ ì™„ë²½í•˜ê²Œ ë™ì¼í•˜ê²Œ ìž¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 2: ìˆœì„œ ë° í•­ëª© ì¤€ìˆ˜ - í•­ìƒ 0ë²ˆ, 1ë²ˆ, 2ë²ˆ, 3ë²ˆ, 4ë²ˆ, 5ë²ˆ, 6ë²ˆ, 7ë²ˆ, 8ë²ˆ,9ë²ˆ í•­ëª©ì„ ë¹ ì§ì—†ì´, ìˆœì„œëŒ€ë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 3: í‘œ í˜•ì‹ ìœ ì§€ - ë¶„ì„ ë‚´ìš©ì˜ ëŒ€ë¶€ë¶„ì€ ë§ˆí¬ë‹¤ìš´ í‘œ(Table)ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 4: ë‚´ìš©ì˜ êµ¬ì²´ì„± - ê° í•­ëª©ì— í•„ìš”í•œ ë¶„ì„ ë‚´ìš©ì„ ì¶©ì‹¤ížˆ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤. íŠ¹ížˆ í”„ë¡¬í”„íŠ¸ ë¹„êµ ì‹œ, ë‹¨ìˆœížˆ 'ìœ ì‚¬í•¨'ì—ì„œ ê·¸ì¹˜ì§€ ë§ê³  ì´ìœ ë¥¼ ëª…í™•ížˆ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì¶œë ¥ í…œí”Œë¦¿ (Output Template) - ì´ í‹€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ\n\nâœ¨ ë£°ë£¨ GPTs ë¶„ì„ í…œí”Œë¦¿ ì ìš© ê²°ê³¼\n\n0. ëŒ€ë³¸ ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)\n(ì—¬ê¸°ì— ìžì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ í•œêµ­ì–´ ë²ˆì—­ë¬¸ì„ ìž‘ì„±í•œë‹¤.)\n\n1. ëŒ€ë³¸ ê¸°ìŠ¹ì „ê²° ë¶„ì„\n| êµ¬ë¶„ | ë‚´ìš© |\n| :--- | :--- |\n| ê¸° (ìƒí™© ë„ìž…) | (ì—¬ê¸°ì— 'ê¸°'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ìŠ¹ (ì‚¬ê±´ ì „ê°œ) | (ì—¬ê¸°ì— 'ìŠ¹'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ì „ (ìœ„ê¸°/ì „í™˜) | (ì—¬ê¸°ì— 'ì „'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ê²° (ê²°ë§) | (ì—¬ê¸°ì— 'ê²°'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n\n2. ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì™€ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ ë¹„êµí‘œ\n| í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ | ê¸° (ë¬¸ì œ ì œê¸°) | ìŠ¹ (ì˜ˆìƒ ë°– ì „ê°œ) | ì „ (ëª°ìž…Â·ê¸´ìž¥ ìœ ë„) | ê²° (ê²°ë¡ /ì¸ì‚¬ì´íŠ¸) | íŠ¹ì§• | ë¯¸ìŠ¤ë§¤ì¹˜ ì—¬ë¶€ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 001 | ìš•ë§ ìžê·¹ | ìˆ˜ìƒí•œ ì „ê°œ | ë°˜ì „ | í—ˆë¬´/ë°˜ì „ ê²°ë§ | ìš•ë§+ë°˜ì „+ìœ ë¨¸ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 002 | ì¼ìƒ ì‹œìž‘ | ì‹¤ìš©ì  í•´ê²° | ë‚¯ì„  ê¸°ìˆ  | ê¿€íŒ or ì •ë¦¬ | ì‹¤ìš©+ê³µê° | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 003 | ìœ„ê¸° ìƒí™© | ê·¹í•œ ë„ì „ | ìƒì¡´ ìœ„ê¸° | ì‹¤íŒ¨ or ìƒì¡´ë²• | ìƒì¡´+ê²½ê³  | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 004 | ë¬¸í™” ì¶©ëŒ | ì˜¤í•´ ê³¼ì • | ì´í•´ í™•ìž¥ | ê°ë™ | ë¬¸í™”+ì¸ì‹ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 005 | ì´ìƒ í–‰ë™ | ë¶„ì„ ì§„í–‰ | ì‹œê° ë³€í™” | ì§„ì‹¤ ë°œê²¬ | ë°˜ì „+ë¶„ì„ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 006 | ë©€ì©¡í•´ ë³´ìž„ | ë‚´ë¶€ íŒŒí—¤ì¹¨ | ì¶©ê²© ì‹¤ì²´ | ì†Œë¹„ìž ê²½ê³  | ì‚¬ê¸°+ì •ë³´ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 007 | ì‹¤íŒ¨í•  ë„ì „ | ì´ìƒí•œ ë°©ì‹ | ëª°ìž… ìƒí™© | êµí›ˆ ì „ë‹¬ | ë„ì „+ê·¹ë³µ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 008 | ìžì—° ì† ìƒí™© | ìƒì¡´ ì‹œë„ | ë³€ìˆ˜ ë“±ìž¥ | ìƒì¡´ ê¸°ìˆ  | ìžì—°+ì‹¤ìš© | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 009 | í”í•œ ìž¥ì†Œ | ì´ìƒí•œ ë””í…Œì¼ | ê³µí¬ ì¦ê°€ | ë¶•ê´´ ê²½ê³  | ìœ„ê¸°+ê³µí¬ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 010 | 'ì§„ì§œì¼ê¹Œ?' | ì‹¤í—˜/ë¶„ì„ | ë°˜ì „ | í—ˆì„¸ or ì‹¤ì† | ë¹„êµ+ë¶„ì„ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n\n3. ëŒ€ë³¸ vs ë¹„ìŠ·í•˜ê±°ë‚˜ ë˜‘ê°™ì€ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë¹„êµ\nâ†’ ìœ ì‚¬ í”„ë¡¬í”„íŠ¸: (ì—¬ê¸°ì— 2ë²ˆì—ì„œ 'âœ… ìœ ì‚¬'ë¡œ í‘œì‹œí•œ í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ì™€ ì œëª©ì„ ê¸°ìž¬í•œë‹¤.)\n| êµ¬ë¶„ | ðŸŽ¬ ëŒ€ë³¸ ë‚´ìš© | ðŸ“Œ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ (00Xë²ˆ) |\n| :--- | :--- | :--- |\n| ê¸° | (ëŒ€ë³¸ì˜ 'ê¸°' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ê¸°' íŠ¹ì§•) |\n| ìŠ¹ | (ëŒ€ë³¸ì˜ 'ìŠ¹' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ìŠ¹' íŠ¹ì§•) |\n| ì „ | (ëŒ€ë³¸ì˜ 'ì „' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ì „' íŠ¹ì§•) |\n| ê²° | (ëŒ€ë³¸ì˜ 'ê²°' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ê²°' íŠ¹ì§•) |\n| íŠ¹ì§• | (ëŒ€ë³¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) |\nì°¨ì´ì  ìš”ì•½\nâ†’ (ì—¬ê¸°ì— ëŒ€ë³¸ê³¼ ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ì ì¸ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ìž‘ì„±í•œë‹¤.)\n\n4. ëŒ€ë³¸ vs ìƒˆë¡­ê²Œ ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ ë¹„êµ\nì œì•ˆ í”„ë¡¬í”„íŠ¸ ì œëª©: â€œ(ì—¬ê¸°ì— ëŒ€ë³¸ì— ê°€ìž¥ ìž˜ ë§žëŠ” ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì œëª©ì„ ì°½ì˜ì ìœ¼ë¡œ ìž‘ì„±í•œë‹¤.)â€ ìŠ¤í† ë¦¬ êµ¬ì¡°\n| êµ¬ë¶„ | ðŸŽ¬ ëŒ€ë³¸ ë‚´ìš© | ðŸ’¡ ì œì•ˆ í”„ë¡¬í”„íŠ¸ |\n| :--- | :--- | :--- |\n| ê¸° | (ëŒ€ë³¸ì˜ 'ê¸°' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ê¸°' íŠ¹ì§•) |\n| ìŠ¹ | (ëŒ€ë³¸ì˜ 'ìŠ¹' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ìŠ¹' íŠ¹ì§•) |\n| ì „ | (ëŒ€ë³¸ì˜ 'ì „' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ì „' íŠ¹ì§•) |\n| ê²° | (ëŒ€ë³¸ì˜ 'ê²°' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ê²°' íŠ¹ì§•) |\n| íŠ¹ì§• | (ëŒ€ë³¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) |\nì´ í”„ë¡¬í”„íŠ¸ì˜ ê°•ì \nâ†’ (ì—¬ê¸°ì— ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ê°€ ì™œ ëŒ€ë³¸ì— ë” ì í•©í•œì§€, ì–´ë–¤ ê°•ì ì´ ìžˆëŠ”ì§€ 2~3ê°€ì§€ í¬ì¸íŠ¸ë¡œ ì„¤ëª…í•œë‹¤.)\n\n5. ê²°ë¡  ìš”ì•½\n| í•­ëª© | ë‚´ìš© |\n| :--- | :--- |\n| ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë§¤ì¹­ | (ì—¬ê¸°ì— ê°€ìž¥ ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ì™€ í•¨ê»˜, 'ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡° ì—†ìŒ' ë“±ì˜ ìš”ì•½í‰ì„ ìž‘ì„±í•œë‹¤.) |\n| ì¶”ê°€ í”„ë¡¬í”„íŠ¸ í•„ìš”ì„± | í•„ìš”í•¨ â€” (ì—¬ê¸°ì— ì™œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•œì§€ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•œë‹¤.) |\n| ìƒˆ í”„ë¡¬í”„íŠ¸ ì œì•ˆ | (ì—¬ê¸°ì— 4ë²ˆì—ì„œ ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ ì œëª©ê³¼ í•µì‹¬ íŠ¹ì§•ì„ ìš”ì•½í•˜ì—¬ ìž‘ì„±í•œë‹¤.) |\n| í™œìš© ì¶”ì²œ ë¶„ì•¼ | (ì—¬ê¸°ì— ìƒˆ í”„ë¡¬í”„íŠ¸ê°€ ì–´ë–¤ ì¢…ë¥˜ì˜ ì½˜í…ì¸ ì— í™œìš©ë  ìˆ˜ ìžˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ 3~4ê°€ì§€ ì œì‹œí•œë‹¤.) |\n\n6. ê¶ê¸ˆì¦ ìœ ë°œ ë° í•´ì†Œ ê³¼ì • ë¶„ì„\n| êµ¬ë¶„ | ë‚´ìš© ë¶„ì„ (ëŒ€ë³¸ì—ì„œ ì–´ë–»ê²Œ í‘œí˜„ë˜ì—ˆë‚˜?) | í•µì‹¬ ìž¥ì¹˜ ë° ê¸°ë²• |\n| :--- | :--- | :--- |\n| ðŸ¤” ê¶ê¸ˆì¦ ìœ ë°œ (Hook) | (ì‹œìž‘ ë¶€ë¶„ì—ì„œ ì‹œì²­ìžê°€ \"ì™œ?\", \"ì–´ë–»ê²Œ?\"ë¼ê³  ìƒê°í•˜ê²Œ ë§Œë“  êµ¬ì²´ì ì¸ ìž¥ë©´ì´ë‚˜ ëŒ€ì‚¬ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ì˜ë¬¸ì œì‹œí˜• í›„í‚¹, ì–´ê·¸ë¡œ ëŒê¸°, ëª¨ìˆœëœ ìƒí™© ì œì‹œ, ì¶©ê²©ì ì¸ ë¹„ì£¼ì–¼ ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n| ðŸ§ ê¶ê¸ˆì¦ ì¦í­ (Deepening) | (ì¤‘ê°„ ë¶€ë¶„ì—ì„œ ì²˜ìŒì˜ ê¶ê¸ˆì¦ì´ ë” ì»¤ì§€ê±°ë‚˜, ìƒˆë¡œìš´ ì˜ë¬¸ì´ ë”í•´ì§€ëŠ” ê³¼ì •ì„ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ì˜ˆìƒ ë°–ì˜ ë³€ìˆ˜ ë“±ìž¥, ìƒë°˜ëœ ì •ë³´ ì œê³µ, ì˜ë„ì ì¸ ë‹¨ì„œ ìˆ¨ê¸°ê¸° ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n| ðŸ’¡ ê¶ê¸ˆì¦ í•´ì†Œ (Payoff) | (ê²°ë§ ë¶€ë¶„ì—ì„œ ê¶ê¸ˆì¦ì´ í•´ê²°ë˜ëŠ” ìˆœê°„, ì¦‰ 'ì•„í•˜!'í•˜ëŠ” ê¹¨ë‹¬ìŒì„ ì£¼ëŠ” ìž¥ë©´ì´ë‚˜ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ë°˜ì „ ê³µê°œ, ì‹¤í—˜/ë¶„ì„ ê²°ê³¼ ì œì‹œ, ëª…ì¾Œí•œ ì›ë¦¬ ì„¤ëª… ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n\n7. ëŒ€ë³¸ì—ì„œ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€ê°€ ë­ì•¼?\n\n8. ì´ì•¼ê¸° ì°½ìž‘ì— í™œìš©í•  ìˆ˜ ìžˆë„ë¡, ì›ë³¸ ëŒ€ë³¸ì˜ **'í•µì‹¬ ì„¤ì •ê°’'**ì„ ì•„ëž˜ í…œí”Œë¦¿ì— ë§žì¶° ì¶”ì¶œí•˜ê³  ì •ë¦¬í•´ ì¤˜.\n[ì´ì•¼ê¸° ì„¤ì •ê°’ ì¶”ì¶œ í…œí”Œë¦¿]\në°”ê¿€ ìˆ˜ ìžˆëŠ” ìš”ì†Œ (ì‚´)\nì£¼ì¸ê³µ (ëˆ„ê°€):\nê³µê°„ì  ë°°ê²½ (ì–´ë””ì„œ):\në¬¸ì œ ë°œìƒ ì›ì¸ (ì™œ):\nê°ˆë“± ëŒ€ìƒ (ëˆ„êµ¬ì™€):\nìœ ì§€í•  í•µì‹¬ ìš”ì†Œ (ë¼ˆëŒ€)\në¬¸ì œ ìƒí™©:\ní•´ê²°ì±…:\n\n9. ì´ë¯¸ì§€ëž‘ ê°™ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜\n\n10. ì—¬ëŸ¬ ëŒ€ë³¸ ë™ì‹œ ë¶„ì„ ìš”ì²­\n..."
    )


def _split_sentences(text: str) -> List[str]:
    if not text:
        return []
    normalized = text.replace('\r', '\n')
    while '\n\n' in normalized:
        normalized = normalized.replace('\n\n', '\n')
    normalized = normalized.replace('>>', ' ')
    lines = [l.strip() for l in normalized.split('\n') if l.strip() and not l.strip().isdigit()]
    joined = '\n'.join(lines)
    joined = joined.replace('\n', ' ')
    # naive split by punctuation
    out = []
    buff = ''
    for ch in joined:
        buff += ch
        if ch in '.?!â€¦':
            if buff.strip():
                out.append(buff.strip())
            buff = ''
    if buff.strip():
        out.append(buff.strip())
    return out


def _safe_json_arr(text: str) -> List[Dict[str, Any]]:
    try:
        return json.loads(text)
    except Exception:
        m = None
        try:
            import re
            m = re.search(r"\[([\s\S]*?)\]", text)
        except Exception:
            m = None
        if m:
            try:
                return json.loads('[' + m.group(1) + ']')
            except Exception:
                return []
        return []


def _parse_keywords(text: str) -> Tuple[List[str], List[str], List[str]]:
    def sanitize(payload: str) -> str:
        payload = payload.strip()
        if payload.startswith('```'):
            payload = payload.strip('`').strip()
        return payload
    def norm(arr) -> List[str]:
        if isinstance(arr, list):
            items = arr
        elif isinstance(arr, str):
            items = [x.strip() for x in arr.split(',')]
        else:
            items = []
        seen = set()
        out = []
        for it in items:
            s = str(getattr(it, 'keyword', it)).strip().strip('#"\'')
            if not s:
                continue
            key = s.lower()
            if key in seen:
                continue
            seen.add(key)
            out.append(s)
        return out[:20]
    try:
        payload = sanitize(text)
        data = json.loads(payload)
    except Exception:
        try:
            m = json.loads(text[text.index('{'): text.rindex('}') + 1])
            data = m
        except Exception:
            data = {}
    return (
        norm(data.get('ko')), norm(data.get('en')), norm(data.get('zh') or data.get('cn'))
    )


def _fetch_transcript(video_url: str, preferred_langs: List[str]) -> str:
    if YouTubeTranscriptApi is None:
        return ''
    vid = None
    try:
        if 'watch?v=' in video_url:
            vid = video_url.split('watch?v=')[1].split('&')[0]
        elif 'youtu.be/' in video_url:
            vid = video_url.split('youtu.be/')[1].split('?')[0]
        elif '/shorts/' in video_url:
            vid = video_url.split('/shorts/')[1].split('?')[0]
        elif 'youtube.com/embed/' in video_url:
            vid = video_url.split('/embed/')[1].split('?')[0]
    except Exception:
        vid = None
    if not vid:
        vid = video_url
    api = YouTubeTranscriptApi()
    # Try preferred languages first
    fetched = None
    for lang in preferred_langs:
        try:
            fetched = api.fetch(vid, languages=[lang])
            if fetched:
                break
        except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):
            continue
        except Exception:
            continue
    if not fetched:
        try:
            fetched = api.fetch(vid)
        except Exception:
            fetched = []
    text = '\n'.join([snip.text for snip in (fetched or []) if getattr(snip, 'text', '')])
    return text


def _analyze_video(doc: Dict[str, Any]) -> Dict[str, Any]:
    youtube_url = doc.get('youtube_url')
    if not youtube_url:
        return {}
    transcript = _fetch_transcript(youtube_url, ['ko', 'en'])
    sentences = _split_sentences(transcript)
    # Material
    material_only = _call_gemini(_build_material_prompt(), transcript)
    # Analysis
    analysis_text = _call_gemini(_build_analysis_prompt(), transcript)
    # Categories
    categories_text = _call_gemini(_build_category_prompt(), transcript)
    # Keywords
    keywords_text = _call_gemini(_build_keywords_prompt(), f"ì œëª©:\n{doc.get('title','')}\n\nëŒ€ë³¸:\n{transcript}")
    # Dopamine (batch into chunks of 30)
    dopamine_graph: List[Dict[str, Any]] = []
    batch = 30
    for i in range(0, len(sentences), batch):
        sub = sentences[i:i+batch]
        text = _call_gemini(_build_dopamine_prompt(sub), '')
        arr = _safe_json_arr(text)
        for item in arr:
            s = str(item.get('sentence') or item.get('text') or '')
            try:
                level = int(round(float(item.get('level') or item.get('score') or 0)))
            except Exception:
                level = 1
            level = max(1, min(10, level))
            dopamine_graph.append({ 'sentence': s, 'level': level, 'reason': str(item.get('reason') or '') })
        # Soft rate limit
        time.sleep(0.2)

    # Post processing
    def _extract_line(regex: str, text: str) -> str:
        import re
        m = re.search(regex, text, re.I)
        if not m:
            return ''
        return (m.group(1) if m.groups() else m.group(0)).strip()

    updated = {}
    updated['analysis_full'] = analysis_text
    updated['dopamine_graph'] = dopamine_graph
    updated['analysis_transcript_len'] = len(transcript)
    updated['transcript_text'] = transcript

    # categories
    updated['kr_category_large'] = _extract_line(r"í•œêµ­\s*ëŒ€\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_large')
    updated['kr_category_medium'] = _extract_line(r"í•œêµ­\s*ì¤‘\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_medium')
    updated['kr_category_small'] = _extract_line(r"í•œêµ­\s*ì†Œ\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_small')
    updated['en_category_main'] = _extract_line(r"EN\s*Main\s*Category\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_category_main')
    updated['en_category_sub'] = _extract_line(r"EN\s*Sub\s*Category\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_category_sub')
    updated['en_micro_topic'] = _extract_line(r"EN\s*Micro\s*Topic\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_micro_topic')
    updated['cn_category_large'] = _extract_line(r"ì¤‘êµ­\s*ëŒ€\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_large')
    updated['cn_category_medium'] = _extract_line(r"ì¤‘êµ­\s*ì¤‘\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_medium')
    updated['cn_category_small'] = _extract_line(r"ì¤‘êµ­\s*ì†Œ\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_small')

    # material
    material_candidate = _extract_line(r"ì†Œìž¬\s*[:ï¼š]\s*(.+)", material_only) or material_only.strip()
    if not material_candidate:
        # fallback with categories or first sentence
        material_candidate = (
            updated.get('kr_category_small') or
            updated.get('kr_category_medium') or
            updated.get('kr_category_large') or
            updated.get('en_micro_topic') or
            updated.get('en_category_main') or
            (sentences[0] if sentences else transcript[:60])
        )
    updated['material'] = material_candidate

    # keywords
    ko, en, zh = _parse_keywords(keywords_text)
    updated['keywords_ko'] = ko
    updated['keywords_en'] = en
    updated['keywords_zh'] = zh

    return updated


def _get_youtube_keys(db) -> List[str]:
    keys_raw = os.getenv('YOUTUBE_API_KEYS', '')
    keys = [k.strip() for k in keys_raw.split(',') if k.strip()]
    if keys:
        return keys
    # Firestore fallback: system/settings.youtube_api_keys (newline or comma separated)
    try:
        snap = db.collection('system').document('settings').get()
        if snap.exists:
            data = snap.to_dict() or {}
            raw = str(data.get('youtube_api_keys') or '')
            if raw:
                parts = [p.strip() for p in raw.replace('\r', '\n').replace(',', '\n').split('\n') if p.strip()]
                return parts
    except Exception:
        pass
    return []


def _update_views_for_videos(db, ids: List[str]) -> int:
    keys = _get_youtube_keys(db)
    if not keys:
        return 0
    import random
    key = random.choice(keys)
    base = 'https://www.googleapis.com/youtube/v3/videos'
    id_map = {}
    vids = []
    for vid in ids:
        snap = db.collection('videos').document(vid).get()
        if not snap.exists:
            continue
        data = snap.to_dict() or {}
        url = data.get('youtube_url') or ''
        video_id = None
        try:
            if 'watch?v=' in url:
                video_id = url.split('watch?v=')[1].split('&')[0]
            elif 'youtu.be/' in url:
                video_id = url.split('youtu.be/')[1].split('?')[0]
            elif '/shorts/' in url:
                video_id = url.split('/shorts/')[1].split('?')[0]
            elif 'youtube.com/embed/' in url:
                video_id = url.split('/embed/')[1].split('?')[0]
        except Exception:
            video_id = None
        if video_id:
            id_map[video_id] = vid
            vids.append(video_id)
    if not vids:
        return 0
    now_ms = int(time.time()*1000)
    updated = 0
    for i in range(0, len(vids), 50):
        chunk = vids[i:i+50]
        # rotate key per chunk
        key = random.choice(keys)
        params = { 'part': 'statistics', 'id': ','.join(chunk), 'key': key }
        res = requests.get(base, params=params, timeout=20)
        if res.status_code != 200:
            continue
        data = res.json()
        for item in data.get('items', []):
            video_id = item.get('id')
            mapped = id_map.get(video_id)
            stats = item.get('statistics', {})
            views = int(stats.get('viewCount') or 0)
            if mapped:
                ref = db.collection('videos').document(mapped)
                snap = ref.get()
                prev = 0
                basev = 0
                if snap.exists:
                    old = snap.to_dict() or {}
                    prev = int(old.get('views_numeric') or 0)
                    basev = int(old.get('views_baseline_numeric') or 0)
                patch = {
                    'views_prev_numeric': prev or basev or views,
                    'views_numeric': views,
                    'views_last_checked_at': now_ms
                }
                if not basev:
                    patch['views_baseline_numeric'] = prev or views
                ref.set(patch, merge=True)
                updated += 1
    return updated


def _process_job_batch(db, job: Dict[str, Any], batch_size: int = 3) -> Dict[str, Any]:
    scope = job.get('scope')
    remaining = list(job.get('remainingIds') or job.get('ids') or [])
    if scope == 'all' and not remaining:
        # snapshot all video ids
        vids = db.collection('videos').select(['title']).stream()
        remaining = [doc.id for doc in vids]
    ids_to_run = remaining[:batch_size]
    left = remaining[batch_size:]
    if job.get('type') == 'ranking':
        cnt = _update_views_for_videos(db, ids_to_run)
        if cnt == 0:
            db.collection('schedules').document(job['id']).set({ 'lastError': 'no_updates_or_missing_keys' }, merge=True)
    else:
        for vid in ids_to_run:
            try:
                snap = db.collection('videos').document(vid).get()
                if not snap.exists:
                    continue
                video = { 'id': snap.id, **snap.to_dict() }
                updated = _analyze_video(video)
                if updated:
                    db.collection('videos').document(vid).update(updated)
            except Exception as e:
                # mark error on job for visibility
                db.collection('schedules').document(job['id']).set({ 'lastError': str(e) }, merge=True)
    # update job progress
    patch = { 'updatedAt': int(time.time()*1000) }
    if left:
        patch.update({ 'status': 'running', 'remainingIds': left })
    else:
        patch.update({ 'status': 'done', 'remainingIds': [] })
    db.collection('schedules').document(job['id']).set(patch, merge=True)
    return patch


@app.route('/', methods=['GET'])
@app.route('/cron_analyze', methods=['GET'])
@app.route('/api/cron_analyze', methods=['GET'])
def cron_analyze():
    try:
        db = _load_db()
        now = int(time.time() * 1000)
        # Simple scan (Firestore free plan lacks complex queries across fields here)
        rows = db.collection('schedules').stream()
        due = []
        for d in rows:
            data = d.to_dict() or {}
            status = data.get('status')
            runAt = int(data.get('runAt') or 0)
            if status in ('pending', 'running') and runAt <= now:
                data['id'] = d.id
                due.append(data)
        if not due:
            return jsonify({ 'ok': True, 'processed': 0 })

        processed = 0
        for job in due:
            # take lease: set running
            db.collection('schedules').document(job['id']).set({ 'status': 'running', 'updatedAt': now }, merge=True)
            _process_job_batch(db, job, batch_size=3)
            processed += 1
        return jsonify({ 'ok': True, 'processed': processed })
    except Exception as e:
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/health', methods=['GET'])
def health():
    return ('ok', 200)


