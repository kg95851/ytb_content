import os
import json
import time
from typing import List, Dict, Any, Tuple

from flask import Flask, jsonify, request
import requests

try:
    from supabase import create_client, Client
except Exception:
    create_client = None
    Client = None

try:
    from youtube_transcript_api import (
        YouTubeTranscriptApi,
        TranscriptsDisabled,
        NoTranscriptFound,
        VideoUnavailable,
    )
except Exception:
    YouTubeTranscriptApi = None


app = Flask(__name__)


@app.after_request
def add_cors_headers(resp):
    try:
        resp.headers['Access-Control-Allow-Origin'] = '*'
        resp.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
        resp.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
    except Exception:
        pass
    return resp


def _load_sb() -> "Client":
    if create_client is None:
        raise RuntimeError('supabase client not available')
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_SERVICE_ROLE_KEY') or os.getenv('SUPABASE_ANON_KEY')
    if not url or not key:
        raise RuntimeError('Missing SUPABASE_URL or SUPABASE_*_KEY env')
    return create_client(url, key)


def _call_gemini(system_prompt: str, user_content: str) -> str:
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise RuntimeError('GEMINI_API_KEY not set')
    prefer = os.getenv('GEMINI_MODEL')
    candidates = [
        *( [prefer] if prefer else [] ),
        'models/gemini-2.5-flash',
        'models/gemini-2.0-flash-exp',
        'models/gemini-1.5-flash-latest',
        'models/gemini-1.5-pro-latest',
    ]
    payload = {
        'contents': [
            { 'role': 'user', 'parts': [{ 'text': f"{system_prompt}\n\n{user_content}" }] }
        ],
        'generationConfig': { 'temperature': 0.3 }
    }
    base = 'https://generativelanguage.googleapis.com'
    errors = []
    for api_ver in ('v1', 'v1beta'):
        for model in candidates:
            if not model:
                continue
            url = f"{base}/{api_ver}/{model}:generateContent?key={api_key}"
            try:
                res = requests.post(url, json=payload, timeout=90)
                if res.status_code == 404:
                    errors.append(f"{api_ver}/{model}:404")
                    continue
                res.raise_for_status()
                data = res.json()
                text = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                if text:
                    return text
            except Exception as e:
                errors.append(f"{api_ver}/{model}:{str(e)[:80]}")
                continue
    raise RuntimeError('Gemini request failed: ' + '; '.join(errors))


def _build_category_prompt() -> str:
    return (
        'ë‹¤ìŒ ëŒ€ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ì•„ëž˜ í˜•ì‹ìœ¼ë¡œë§Œ í•œ ì¤„ì”© ì •í™•ížˆ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸/ë¨¸ë¦¬ë§/ì„¤ëª… ê¸ˆì§€.\n'
        'í•œêµ­ ëŒ€ ì¹´í…Œê³ ë¦¬: \ní•œêµ­ ì¤‘ ì¹´í…Œê³ ë¦¬: \ní•œêµ­ ì†Œ ì¹´í…Œê³ ë¦¬: \n'
        'EN Main Category: \nEN Sub Category: \nEN Micro Topic: \n'
        'ì¤‘êµ­ ëŒ€ ì¹´í…Œê³ ë¦¬: \nì¤‘êµ­ ì¤‘ ì¹´í…Œê³ ë¦¬: \nì¤‘êµ­ ì†Œ ì¹´í…Œê³ ë¦¬: '
    )


def _build_keywords_prompt() -> str:
    return (
        'ì•„ëž˜ ì œê³µëœ "ì œëª©"ê³¼ "ëŒ€ë³¸"ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, ì›ë³¸ ì˜ìƒì„ ê²€ìƒ‰í•´ ì°¾ê¸° ì‰¬ìš´ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´/ì˜ì–´/ì¤‘êµ­ì–´ë¡œ ê°ê° 8~15ê°œì”© ì¶”ì¶œí•˜ì„¸ìš”.\n'
        'ì¶œë ¥ í˜•ì‹ì€ JSON ê°ì²´ë§Œ, ë‹¤ë¥¸ ì„¤ëª…/ë¨¸ë¦¬ë§/ì½”ë“œíŽœìŠ¤ ê¸ˆì§€.\n'
        'ìš”êµ¬ í˜•ì‹: {"ko":["í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2",...],"en":["keyword1",...],"zh":["å…³é”®è¯1",...]}\n'
        'ê·œì¹™:\n- ê° í‚¤ì›Œë“œëŠ” 1~4ë‹¨ì–´ì˜ ì§§ì€ êµ¬ë¡œ ìž‘ì„±\n- í•´ì‹œíƒœê·¸/íŠ¹ìˆ˜ë¬¸ìž/ë”°ì˜´í‘œ ì œê±°, ë¶ˆìš©ì–´ ì œì™¸\n- ë™ì¼ ì˜ë¯¸/ì¤‘ë³µ í‘œí˜„ì€ í•˜ë‚˜ë§Œ ìœ ì§€\n- ì¸ëª…/ì±„ë„ëª…/ë¸Œëžœë“œ/í•µì‹¬ ì£¼ì œ í¬í•¨\n'
    )


def _build_material_prompt() -> str:
    return 'ë‹¤ìŒ ëŒ€ë³¸ì˜ í•µì‹¬ ì†Œìž¬ë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ì¤„ë¡œë§Œ, "ì†Œìž¬: "ë¡œ ì‹œìž‘í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ìžëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.'

def _build_hooking_prompt() -> str:
    return (
        'ë‹¤ìŒ ëŒ€ë³¸ì—ì„œ ì‹œì²­ìžì˜ ê´€ì‹¬ì„ ë„ëŠ” í•µì‹¬ í›„í‚¹ ìš”ì†Œë¥¼ 1~3ê°œ ì¶”ë ¤ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìž‘ì„±í•˜ì„¸ìš”.\n'
        'ì¶œë ¥ í˜•ì‹: "í›„í‚¹ ìš”ì†Œ: ..." í•œ ì¤„(ì—¬ëŸ¬ ê°œë©´ \' Â· \'ë¡œ êµ¬ë¶„). ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.'
    )

def _build_structure_prompt() -> str:
    return (
        'ë‹¤ìŒ ëŒ€ë³¸ì„ ê¸°ìŠ¹ì „ê²° êµ¬ì¡°ë¡œ ìš”ì•½í•˜ì„¸ìš”. ê° ë‹¨ê³„ëŠ” 1ë¬¸ìž¥ ì´ë‚´ë¡œ ê°„ê²°ížˆ.\n'
        'ì¶œë ¥ í˜•ì‹:\nê¸°: ...\nìŠ¹: ...\nì „: ...\nê²°: ...\në‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.'
    )

def _build_dopamine_prompt(sentences: List[str]) -> str:
    header = 'ë‹¤ìŒ "ë¬¸ìž¥ ë°°ì—´"ì— ëŒ€í•´, ê° ë¬¸ìž¥ë³„ë¡œ ê¶ê¸ˆì¦/ë„íŒŒë¯¼ ìœ ë°œ ì •ë„ë¥¼ 1~10 ì •ìˆ˜ë¡œ í‰ê°€í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨ížˆ ì„¤ëª…í•˜ì„¸ìš”. ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œë§Œ, ìš”ì†ŒëŠ” {"sentence":"ë¬¸ìž¥","level":ì •ìˆ˜,"reason":"ì´ìœ "} í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”. ì—¬ëŠ” ëŒ€ê´„í˜¸ë¶€í„° ë‹«ëŠ” ëŒ€ê´„í˜¸ê¹Œì§€ ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.'
    return header + '\n\në¬¸ìž¥ ë°°ì—´:\n' + json.dumps(sentences, ensure_ascii=False)


def _build_analysis_prompt() -> str:
    # ì¶•ì•½ ì—†ì´ ë™ì¼ í…œí”Œë¦¿ ìœ ì§€
    return (
        "[GPTs Instructions ìµœì¢…ì•ˆ]\n\níŽ˜ë¥´ì†Œë‚˜ (Persona)\n\në‹¹ì‹ ì€ \"ëŒ€ë³¸ë¶„ì„_ë£°ë£¨ëž„ë¼ë¦´ë¦¬\"ìž…ë‹ˆë‹¤. ìœ íŠœë¸Œ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ ì½˜í…ì¸  ì „ëžµ ìˆ˜ë¦½ê³¼ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ë•ëŠ” ìµœê³ ì˜ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë‹µë³€ì€ í•­ìƒ ì²´ê³„ì ì´ê³ , ê¹”ë”í•˜ë©°, ì‚¬ìš©ìžê°€ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìžˆë„ë¡ ì™„ë²½í•˜ê²Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n\ní•µì‹¬ ìž„ë¬´ (Core Mission)\n\nì‚¬ìš©ìžê°€ ìœ íŠœë¸Œ ëŒ€ë³¸(ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´)ì„ ìž…ë ¥í•˜ë©´, ì•„ëž˜ 4ë²ˆ í•­ëª©ì˜ **[ì¶œë ¥ í…œí”Œë¦¿]**ì„ ë‹¨ í•˜ë‚˜ì˜ ê¸€ìžë‚˜ ê¸°í˜¸ë„ í‹€ë¦¬ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n\nì ˆëŒ€ ê·œì¹™ (Golden Rules)\n\nê·œì¹™ 1: í…œí”Œë¦¿ ë³µì œ - ì¶œë ¥ë¬¼ì˜ êµ¬ì¡°, ë””ìžì¸, ìˆœì„œ, í•­ëª© ë²ˆí˜¸, ì´ëª¨ì§€(âœ¨, ðŸ“Œ, ðŸŽ¬, ðŸ§, ðŸ’¡, âœ…, ðŸ¤”), ê°•ì¡°(), êµ¬ë¶„ì„ (*) ë“± ëª¨ë“  ì‹œê°ì  ìš”ì†Œë¥¼ ì•„ëž˜ **[ì¶œë ¥ í…œí”Œë¦¿]**ê³¼ ì™„ë²½í•˜ê²Œ ë™ì¼í•˜ê²Œ ìž¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 2: ìˆœì„œ ë° í•­ëª© ì¤€ìˆ˜ - í•­ìƒ 0ë²ˆ, 1ë²ˆ, 2ë²ˆ, 3ë²ˆ, 4ë²ˆ, 5ë²ˆ, 6ë²ˆ, 7ë²ˆ, 8ë²ˆ,9ë²ˆ í•­ëª©ì„ ë¹ ì§ì—†ì´, ìˆœì„œëŒ€ë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 3: í‘œ í˜•ì‹ ìœ ì§€ - ë¶„ì„ ë‚´ìš©ì˜ ëŒ€ë¶€ë¶„ì€ ë§ˆí¬ë‹¤ìš´ í‘œ(Table)ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·œì¹™ 4: ë‚´ìš©ì˜ êµ¬ì²´ì„± - ê° í•­ëª©ì— í•„ìš”í•œ ë¶„ì„ ë‚´ìš©ì„ ì¶©ì‹¤ížˆ ì±„ì›Œì•¼ í•©ë‹ˆë‹¤. íŠ¹ížˆ í”„ë¡¬í”„íŠ¸ ë¹„êµ ì‹œ, ë‹¨ìˆœížˆ 'ìœ ì‚¬í•¨'ì—ì„œ ê·¸ì¹˜ì§€ ë§ê³  ì´ìœ ë¥¼ ëª…í™•ížˆ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì¶œë ¥ í…œí”Œë¦¿ (Output Template) - ì´ í‹€ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ\n\nâœ¨ ë£°ë£¨ GPTs ë¶„ì„ í…œí”Œë¦¿ ì ìš© ê²°ê³¼\n\n0. ëŒ€ë³¸ ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)\n(ì—¬ê¸°ì— ìžì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ í•œêµ­ì–´ ë²ˆì—­ë¬¸ì„ ìž‘ì„±í•œë‹¤.)\n\n1. ëŒ€ë³¸ ê¸°ìŠ¹ì „ê²° ë¶„ì„\n| êµ¬ë¶„ | ë‚´ìš© |\n| :--- | :--- |\n| ê¸° (ìƒí™© ë„ìž…) | (ì—¬ê¸°ì— 'ê¸°'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ìŠ¹ (ì‚¬ê±´ ì „ê°œ) | (ì—¬ê¸°ì— 'ìŠ¹'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ì „ (ìœ„ê¸°/ì „í™˜) | (ì—¬ê¸°ì— 'ì „'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n| ê²° (ê²°ë§) | (ì—¬ê¸°ì— 'ê²°'ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ìš”ì•½í•œë‹¤.) |\n\n2. ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì™€ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ ë¹„êµí‘œ\n| í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ | ê¸° (ë¬¸ì œ ì œê¸°) | ìŠ¹ (ì˜ˆìƒ ë°– ì „ê°œ) | ì „ (ëª°ìž…Â·ê¸´ìž¥ ìœ ë„) | ê²° (ê²°ë¡ /ì¸ì‚¬ì´íŠ¸) | íŠ¹ì§• | ë¯¸ìŠ¤ë§¤ì¹˜ ì—¬ë¶€ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| 001 | ìš•ë§ ìžê·¹ | ìˆ˜ìƒí•œ ì „ê°œ | ë°˜ì „ | í—ˆë¬´/ë°˜ì „ ê²°ë§ | ìš•ë§+ë°˜ì „+ìœ ë¨¸ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 002 | ì¼ìƒ ì‹œìž‘ | ì‹¤ìš©ì  í•´ê²° | ë‚¯ì„  ê¸°ìˆ  | ê¿€íŒ or ì •ë¦¬ | ì‹¤ìš©+ê³µê° | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 003 | ìœ„ê¸° ìƒí™© | ê·¹í•œ ë„ì „ | ìƒì¡´ ìœ„ê¸° | ì‹¤íŒ¨ or ìƒì¡´ë²• | ìƒì¡´+ê²½ê³  | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 004 | ë¬¸í™” ì¶©ëŒ | ì˜¤í•´ ê³¼ì • | ì´í•´ í™•ìž¥ | ê°ë™ | ë¬¸í™”+ì¸ì‹ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 005 | ì´ìƒ í–‰ë™ | ë¶„ì„ ì§„í–‰ | ì‹œê° ë³€í™” | ì§„ì‹¤ ë°œê²¬ | ë°˜ì „+ë¶„ì„ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 006 | ë©€ì©¡í•´ ë³´ìž„ | ë‚´ë¶€ íŒŒí—¤ì¹¨ | ì¶©ê²© ì‹¤ì²´ | ì†Œë¹„ìž ê²½ê³  | ì‚¬ê¸°+ì •ë³´ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 007 | ì‹¤íŒ¨í•  ë„ì „ | ì´ìƒí•œ ë°©ì‹ | ëª°ìž… ìƒí™© | êµí›ˆ ì „ë‹¬ | ë„ì „+ê·¹ë³µ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 008 | ìžì—° ì† ìƒí™© | ìƒì¡´ ì‹œë„ | ë³€ìˆ˜ ë“±ìž¥ | ìƒì¡´ ê¸°ìˆ  | ìžì—°+ì‹¤ìš© | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 009 | í”í•œ ìž¥ì†Œ | ì´ìƒí•œ ë””í…Œì¼ | ê³µí¬ ì¦ê°€ | ë¶•ê´´ ê²½ê³  | ìœ„ê¸°+ê³µí¬ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n| 010 | 'ì§„ì§œì¼ê¹Œ?' | ì‹¤í—˜/ë¶„ì„ | ë°˜ì „ | í—ˆì„¸ or ì‹¤ì† | ë¹„êµ+ë¶„ì„ | (ëŒ€ë³¸ê³¼ ë¹„êµí•˜ì—¬ âœ… ë˜ëŠ” âŒ ìœ ì‚¬ë¡œ í‘œì‹œ) |\n\n3. ëŒ€ë³¸ vs ë¹„ìŠ·í•˜ê±°ë‚˜ ë˜‘ê°™ì€ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë¹„êµ\nâ†’ ìœ ì‚¬ í”„ë¡¬í”„íŠ¸: (ì—¬ê¸°ì— 2ë²ˆì—ì„œ 'âœ… ìœ ì‚¬'ë¡œ í‘œì‹œí•œ í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ì™€ ì œëª©ì„ ê¸°ìž¬í•œë‹¤.)\n| êµ¬ë¶„ | ðŸŽ¬ ëŒ€ë³¸ ë‚´ìš© | ðŸ“Œ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ (00Xë²ˆ) |\n| :--- | :--- | :--- |\n| ê¸° | (ëŒ€ë³¸ì˜ 'ê¸°' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ê¸°' íŠ¹ì§•) |\n| ìŠ¹ | (ëŒ€ë³¸ì˜ 'ìŠ¹' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ìŠ¹' íŠ¹ì§•) |\n| ì „ | (ëŒ€ë³¸ì˜ 'ì „' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ì „' íŠ¹ì§•) |\n| ê²° | (ëŒ€ë³¸ì˜ 'ê²°' ìš”ì•½) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ 'ê²°' íŠ¹ì§•) |\n| íŠ¹ì§• | (ëŒ€ë³¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) | (ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) |\nì°¨ì´ì  ìš”ì•½\nâ†’ (ì—¬ê¸°ì— ëŒ€ë³¸ê³¼ ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ì ì¸ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ìž‘ì„±í•œë‹¤.)\n\n4. ëŒ€ë³¸ vs ìƒˆë¡­ê²Œ ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ ë¹„êµ\nì œì•ˆ í”„ë¡¬í”„íŠ¸ ì œëª©: â€œ(ì—¬ê¸°ì— ëŒ€ë³¸ì— ê°€ìž¥ ìž˜ ë§žëŠ” ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ ì œëª©ì„ ì°½ì˜ì ìœ¼ë¡œ ìž‘ì„±í•œë‹¤.)â€ ìŠ¤í† ë¦¬ êµ¬ì¡°\n| êµ¬ë¶„ | ðŸŽ¬ ëŒ€ë³¸ ë‚´ìš© | ðŸ’¡ ì œì•ˆ í”„ë¡¬í”„íŠ¸ |\n| :--- | :--- | :--- |\n| ê¸° | (ëŒ€ë³¸ì˜ 'ê¸°' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ê¸°' íŠ¹ì§•) |\n| ìŠ¹ | (ëŒ€ë³¸ì˜ 'ìŠ¹' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ìŠ¹' íŠ¹ì§•) |\n| ì „ | (ëŒ€ë³¸ì˜ 'ì „' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ì „' íŠ¹ì§•) |\n| ê²° | (ëŒ€ë³¸ì˜ 'ê²°' ìš”ì•½) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ 'ê²°' íŠ¹ì§•) |\n| íŠ¹ì§• | (ëŒ€ë³¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) | (ìƒˆ í”„ë¡¬í”„íŠ¸ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•) |\nì´ í”„ë¡¬í”„íŠ¸ì˜ ê°•ì \nâ†’ (ì—¬ê¸°ì— ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ê°€ ì™œ ëŒ€ë³¸ì— ë” ì í•©í•œì§€, ì–´ë–¤ ê°•ì ì´ ìžˆëŠ”ì§€ 2~3ê°€ì§€ í¬ì¸íŠ¸ë¡œ ì„¤ëª…í•œë‹¤.)\n\n5. ê²°ë¡  ìš”ì•½\n| í•­ëª© | ë‚´ìš© |\n| :--- | :--- |\n| ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë§¤ì¹­ | (ì—¬ê¸°ì— ê°€ìž¥ ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ ë²ˆí˜¸ì™€ í•¨ê»˜, 'ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” êµ¬ì¡° ì—†ìŒ' ë“±ì˜ ìš”ì•½í‰ì„ ìž‘ì„±í•œë‹¤.) |\n| ì¶”ê°€ í”„ë¡¬í”„íŠ¸ í•„ìš”ì„± | í•„ìš”í•¨ â€” (ì—¬ê¸°ì— ì™œ ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•œì§€ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±í•œë‹¤.) |\n| ìƒˆ í”„ë¡¬í”„íŠ¸ ì œì•ˆ | (ì—¬ê¸°ì— 4ë²ˆì—ì„œ ì œì•ˆí•œ í”„ë¡¬í”„íŠ¸ ì œëª©ê³¼ í•µì‹¬ íŠ¹ì§•ì„ ìš”ì•½í•˜ì—¬ ìž‘ì„±í•œë‹¤.) |\n| í™œìš© ì¶”ì²œ ë¶„ì•¼ | (ì—¬ê¸°ì— ìƒˆ í”„ë¡¬í”„íŠ¸ê°€ ì–´ë–¤ ì¢…ë¥˜ì˜ ì½˜í…ì¸ ì— í™œìš©ë  ìˆ˜ ìžˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ 3~4ê°€ì§€ ì œì‹œí•œë‹¤.) |\n\n6. ê¶ê¸ˆì¦ ìœ ë°œ ë° í•´ì†Œ ê³¼ì • ë¶„ì„\n| êµ¬ë¶„ | ë‚´ìš© ë¶„ì„ (ëŒ€ë³¸ì—ì„œ ì–´ë–»ê²Œ í‘œí˜„ë˜ì—ˆë‚˜?) | í•µì‹¬ ìž¥ì¹˜ ë° ê¸°ë²• |\n| :--- | :--- | :--- |\n| ðŸ¤” ê¶ê¸ˆì¦ ìœ ë°œ (Hook) | (ì‹œìž‘ ë¶€ë¶„ì—ì„œ ì‹œì²­ìžê°€ \"ì™œ?\", \"ì–´ë–»ê²Œ?\"ë¼ê³  ìƒê°í•˜ê²Œ ë§Œë“  êµ¬ì²´ì ì¸ ìž¥ë©´ì´ë‚˜ ëŒ€ì‚¬ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ì˜ë¬¸ì œì‹œí˜• í›„í‚¹, ì–´ê·¸ë¡œ ëŒê¸°, ëª¨ìˆœëœ ìƒí™© ì œì‹œ, ì¶©ê²©ì ì¸ ë¹„ì£¼ì–¼ ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n| ðŸ§ ê¶ê¸ˆì¦ ì¦í­ (Deepening) | (ì¤‘ê°„ ë¶€ë¶„ì—ì„œ ì²˜ìŒì˜ ê¶ê¸ˆì¦ì´ ë” ì»¤ì§€ê±°ë‚˜, ìƒˆë¡œìš´ ì˜ë¬¸ì´ ë”í•´ì§€ëŠ” ê³¼ì •ì„ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ì˜ˆìƒ ë°–ì˜ ë³€ìˆ˜ ë“±ìž¥, ìƒë°˜ëœ ì •ë³´ ì œê³µ, ì˜ë„ì ì¸ ë‹¨ì„œ ìˆ¨ê¸°ê¸° ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n| ðŸ’¡ ê¶ê¸ˆì¦ í•´ì†Œ (Payoff) | (ê²°ë§ ë¶€ë¶„ì—ì„œ ê¶ê¸ˆì¦ì´ í•´ê²°ë˜ëŠ” ìˆœê°„, ì¦‰ 'ì•„í•˜!'í•˜ëŠ” ê¹¨ë‹¬ìŒì„ ì£¼ëŠ” ìž¥ë©´ì´ë‚˜ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.) | (ì˜ˆ: ë°˜ì „ ê³µê°œ, ì‹¤í—˜/ë¶„ì„ ê²°ê³¼ ì œì‹œ, ëª…ì¾Œí•œ ì›ë¦¬ ì„¤ëª… ë“± ì‚¬ìš©ëœ ê¸°ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.) |\n\n7. ëŒ€ë³¸ì—ì„œ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€ê°€ ë­ì•¼?\n\n8. ì´ì•¼ê¸° ì°½ìž‘ì— í™œìš©í•  ìˆ˜ ìžˆë„ë¡, ì›ë³¸ ëŒ€ë³¸ì˜ **'í•µì‹¬ ì„¤ì •ê°’'**ì„ ì•„ëž˜ í…œí”Œë¦¿ì— ë§žì¶° ì¶”ì¶œí•˜ê³  ì •ë¦¬í•´ ì¤˜.\n[ì´ì•¼ê¸° ì„¤ì •ê°’ ì¶”ì¶œ í…œí”Œë¦¿]\në°”ê¿€ ìˆ˜ ìžˆëŠ” ìš”ì†Œ (ì‚´)\nì£¼ì¸ê³µ (ëˆ„ê°€):\nê³µê°„ì  ë°°ê²½ (ì–´ë””ì„œ):\në¬¸ì œ ë°œìƒ ì›ì¸ (ì™œ):\nê°ˆë“± ëŒ€ìƒ (ëˆ„êµ¬ì™€):\nìœ ì§€í•  í•µì‹¬ ìš”ì†Œ (ë¼ˆëŒ€)\në¬¸ì œ ìƒí™©:\ní•´ê²°ì±…:\n\n9. ì´ë¯¸ì§€ëž‘ ê°™ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜\n\n10. ì—¬ëŸ¬ ëŒ€ë³¸ ë™ì‹œ ë¶„ì„ ìš”ì²­\n..."
    )


def _split_sentences(text: str) -> List[str]:
    if not text:
        return []
    normalized = text.replace('\r', '\n')
    while '\n\n' in normalized:
        normalized = normalized.replace('\n\n', '\n')
    normalized = normalized.replace('>>', ' ')
    lines = [l.strip() for l in normalized.split('\n') if l.strip() and not l.strip().isdigit()]
    joined = '\n'.join(lines)
    joined = joined.replace('\n', ' ')
    # naive split by punctuation
    out = []
    buff = ''
    for ch in joined:
        buff += ch
        if ch in '.?!â€¦':
            if buff.strip():
                out.append(buff.strip())
            buff = ''
    if buff.strip():
        out.append(buff.strip())
    return out


def _safe_json_arr(text: str) -> List[Dict[str, Any]]:
    try:
        return json.loads(text)
    except Exception:
        m = None
        try:
            import re
            m = re.search(r"\[([\s\S]*?)\]", text)
        except Exception:
            m = None
        if m:
            try:
                return json.loads('[' + m.group(1) + ']')
            except Exception:
                return []
        return []


def _parse_keywords(text: str) -> Tuple[List[str], List[str], List[str]]:
    def sanitize(payload: str) -> str:
        payload = payload.strip()
        if payload.startswith('```'):
            payload = payload.strip('`').strip()
        return payload
    def norm(arr) -> List[str]:
        if isinstance(arr, list):
            items = arr
        elif isinstance(arr, str):
            items = [x.strip() for x in arr.split(',')]
        else:
            items = []
        seen = set()
        out = []
        for it in items:
            s = str(getattr(it, 'keyword', it)).strip().strip('#"\'')
            if not s:
                continue
            key = s.lower()
            if key in seen:
                continue
            seen.add(key)
            out.append(s)
        return out[:20]
    try:
        payload = sanitize(text)
        data = json.loads(payload)
    except Exception:
        try:
            m = json.loads(text[text.index('{'): text.rindex('}') + 1])
            data = m
        except Exception:
            data = {}
    return (
        norm(data.get('ko')), norm(data.get('en')), norm(data.get('zh') or data.get('cn'))
    )


def _fetch_transcript(video_url: str, preferred_langs: List[str]) -> str:
    if YouTubeTranscriptApi is None:
        return ''
    vid = None
    try:
        if 'watch?v=' in video_url:
            vid = video_url.split('watch?v=')[1].split('&')[0]
        elif 'youtu.be/' in video_url:
            vid = video_url.split('youtu.be/')[1].split('?')[0]
        elif '/shorts/' in video_url:
            vid = video_url.split('/shorts/')[1].split('?')[0]
        elif 'youtube.com/embed/' in video_url:
            vid = video_url.split('/embed/')[1].split('?')[0]
    except Exception:
        vid = None
    if not vid:
        vid = video_url
    api = YouTubeTranscriptApi()
    # Try preferred languages first
    fetched = None
    for lang in preferred_langs:
        try:
            fetched = api.fetch(vid, languages=[lang])
            if fetched:
                break
        except (NoTranscriptFound, TranscriptsDisabled, VideoUnavailable):
            continue
        except Exception:
            continue
    if not fetched:
        try:
            fetched = api.fetch(vid)
        except Exception:
            fetched = []
    text = '\n'.join([snip.text for snip in (fetched or []) if getattr(snip, 'text', '')])
    return text


def _analyze_video(doc: Dict[str, Any]) -> Dict[str, Any]:
    youtube_url = doc.get('youtube_url')
    if not youtube_url:
        return {}
    # ìš°ì„  DBì— ì €ìž¥ëœ ëŒ€ë³¸ì„ ì‚¬ìš©í•˜ê³ , ì—†ì„ ë•Œë§Œ ì›ê²© ìžë§‰/ìžë™ìƒì„± ìžë§‰ì„ ì‹œë„
    transcript = str(doc.get('transcript_text') or '').strip()
    if not transcript:
        transcript = _fetch_transcript(youtube_url, ['ko', 'en'])
    sentences = _split_sentences(transcript)
    # Material
    material_only = _call_gemini(_build_material_prompt(), transcript)
    # Hooking & Structure (ê°„ë‹¨ ìš”ì•½)
    hooking_text = _call_gemini(_build_hooking_prompt(), transcript)
    structure_text = _call_gemini(_build_structure_prompt(), transcript)
    # Analysis(ì¹´ë“œ/ì„¸ë¶€)
    analysis_text = _call_gemini(_build_analysis_prompt(), transcript)
    # Categories
    categories_text = _call_gemini(_build_category_prompt(), transcript)
    # Keywords
    keywords_text = _call_gemini(_build_keywords_prompt(), f"ì œëª©:\n{doc.get('title','')}\n\nëŒ€ë³¸:\n{transcript}")
    # Dopamine (batch into chunks of 30)
    dopamine_graph: List[Dict[str, Any]] = []
    batch = 30
    for i in range(0, len(sentences), batch):
        sub = sentences[i:i+batch]
        text = _call_gemini(_build_dopamine_prompt(sub), '')
        arr = _safe_json_arr(text)
        for item in arr:
            s = str(item.get('sentence') or item.get('text') or '')
            try:
                level = int(round(float(item.get('level') or item.get('score') or 0)))
            except Exception:
                level = 1
            level = max(1, min(10, level))
            dopamine_graph.append({ 'sentence': s, 'level': level, 'reason': str(item.get('reason') or '') })
        # Soft rate limit
        time.sleep(0.2)

    # Post processing
    def _extract_line(regex: str, text: str) -> str:
        import re
        m = re.search(regex, text, re.I)
        if not m:
            return ''
        return (m.group(1) if m.groups() else m.group(0)).strip()

    updated = {}
    updated['analysis_full'] = analysis_text
    updated['dopamine_graph'] = dopamine_graph
    updated['analysis_transcript_len'] = len(transcript)
    updated['transcript_text'] = transcript
    # hooking & structure
    if hooking_text:
        updated['hooking'] = hooking_text.strip()[:1000]
    if structure_text:
        updated['narrative_structure'] = structure_text.strip()[:2000]

    # categories
    updated['kr_category_large'] = _extract_line(r"í•œêµ­\s*ëŒ€\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_large')
    updated['kr_category_medium'] = _extract_line(r"í•œêµ­\s*ì¤‘\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_medium')
    updated['kr_category_small'] = _extract_line(r"í•œêµ­\s*ì†Œ\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('kr_category_small')
    updated['en_category_main'] = _extract_line(r"EN\s*Main\s*Category\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_category_main')
    updated['en_category_sub'] = _extract_line(r"EN\s*Sub\s*Category\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_category_sub')
    updated['en_micro_topic'] = _extract_line(r"EN\s*Micro\s*Topic\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('en_micro_topic')
    updated['cn_category_large'] = _extract_line(r"ì¤‘êµ­\s*ëŒ€\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_large')
    updated['cn_category_medium'] = _extract_line(r"ì¤‘êµ­\s*ì¤‘\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_medium')
    updated['cn_category_small'] = _extract_line(r"ì¤‘êµ­\s*ì†Œ\s*ì¹´í…Œê³ ë¦¬\s*[:ï¼š]\s*(.+)", categories_text) or doc.get('cn_category_small')

    # material
    material_candidate = _extract_line(r"ì†Œìž¬\s*[:ï¼š]\s*(.+)", material_only) or material_only.strip()
    if not material_candidate:
        # fallback with categories or first sentence
        material_candidate = (
            updated.get('kr_category_small') or
            updated.get('kr_category_medium') or
            updated.get('kr_category_large') or
            updated.get('en_micro_topic') or
            updated.get('en_category_main') or
            (sentences[0] if sentences else transcript[:60])
        )
    updated['material'] = material_candidate

    # keywords
    ko, en, zh = _parse_keywords(keywords_text)
    updated['keywords_ko'] = ko
    updated['keywords_en'] = en
    updated['keywords_zh'] = zh

    return updated


def _get_youtube_keys(sb) -> List[str]:
    keys_raw = os.getenv('YOUTUBE_API_KEYS', '')
    keys = [k.strip() for k in keys_raw.split(',') if k.strip()]
    if keys:
        return keys
    return []


def _update_views_for_videos(sb, ids: List[str]) -> int:
    keys = _get_youtube_keys(sb)
    if not keys:
        return 0
    import random
    key = random.choice(keys)
    base = 'https://www.googleapis.com/youtube/v3/videos'
    id_map = {}
    vids = []
    for vid in ids:
        # fetch video row from supabase
        res = sb.table('videos').select('*').eq('id', vid).limit(1).execute()
        rows = getattr(res, 'data', []) or []
        if not rows:
            continue
        data = rows[0]
        url = data.get('youtube_url') or ''
        video_id = None
        try:
            if 'watch?v=' in url:
                video_id = url.split('watch?v=')[1].split('&')[0]
            elif 'youtu.be/' in url:
                video_id = url.split('youtu.be/')[1].split('?')[0]
            elif '/shorts/' in url:
                video_id = url.split('/shorts/')[1].split('?')[0]
            elif 'youtube.com/embed/' in url:
                video_id = url.split('/embed/')[1].split('?')[0]
        except Exception:
            video_id = None
        if video_id:
            id_map[video_id] = vid
            vids.append(video_id)
    if not vids:
        return 0
    now_ms = int(time.time()*1000)
    updated = 0
    for i in range(0, len(vids), 50):
        chunk = vids[i:i+50]
        # rotate key per chunk
        key = random.choice(keys)
        params = { 'part': 'statistics', 'id': ','.join(chunk), 'key': key }
        res = requests.get(base, params=params, timeout=20)
        if res.status_code != 200:
            continue
        data = res.json()
        for item in data.get('items', []):
            video_id = item.get('id')
            mapped = id_map.get(video_id)
            stats = item.get('statistics', {})
            views = int(stats.get('viewCount') or 0)
            if mapped:
                prev = 0
                basev = 0
                # read again to compute prev/base
                try:
                    old = data or {}
                    def _parse_human(v):
                        try:
                            if isinstance(v, (int, float)):
                                return int(v)
                            s = str(v or '')
                            digits = ''.join(ch for ch in s if ch.isdigit())
                            return int(digits) if digits else 0
                        except Exception:
                            return 0
                    prev = int(old.get('views_numeric') or 0)
                    basev = int(old.get('views_baseline_numeric') or 0)
                    orig = _parse_human(old.get('views'))
                except Exception:
                    orig = 0
                # ì´ì „ê°’ ê²°ì •: ê¸°ì¡´ current > baseline > import original
                prev_for_patch = prev or basev or orig
                patch = {
                    'views_prev_numeric': prev_for_patch,
                    'views_numeric': views,
                    'views_last_checked_at': now_ms
                }
                if not basev:
                    # ìµœì´ˆ ë² ì´ìŠ¤ë¼ì¸ì€ ê¸°ì¡´ current ë˜ëŠ” import ì›ë³¸
                    patch['views_baseline_numeric'] = prev or orig or views
                sb.table('videos').update(patch).eq('id', mapped).execute()
                updated += 1
    return updated


def _process_job_batch(sb, job: Dict[str, Any], batch_size: int = 3) -> Dict[str, Any]:
    scope = job.get('scope')
    remaining = list(job.get('remaining_ids') or job.get('ids') or job.get('remainingIds') or [])
    if scope == 'all' and not remaining:
        # snapshot all video ids
        res = sb.table('videos').select('id').execute()
        rows = getattr(res, 'data', []) or []
        remaining = [r['id'] for r in rows if r.get('id')]
    ids_to_run = remaining[:batch_size]
    left = remaining[batch_size:]
    if job.get('type') == 'ranking':
        cnt = _update_views_for_videos(sb, ids_to_run)
    else:
        for vid in ids_to_run:
            try:
                res = sb.table('videos').select('*').eq('id', vid).limit(1).execute()
                rows = getattr(res, 'data', []) or []
                if not rows:
                    continue
                video = { 'id': vid, **rows[0] }
                updated = _analyze_video(video)
                if updated:
                    sb.table('videos').update(updated).eq('id', vid).execute()
            except Exception as e:
                # mark error (optional: write to jobs table when exists)
                pass
    # update job progress
    now_iso = __import__('datetime').datetime.utcnow().isoformat() + 'Z'
    patch = { 'updated_at': now_iso }
    if left:
        patch.update({ 'status': 'running', 'remaining_ids': left })
    else:
        patch.update({ 'status': 'done', 'remaining_ids': [] })
    # Try column update; if schema is minimal, merge into content JSON
    try:
        sb.table('schedules').update(patch).eq('id', job['id']).execute()
    except Exception:
        try:
            cur = sb.table('schedules').select('content').eq('id', job['id']).limit(1).execute()
            rows = getattr(cur, 'data', []) or []
            content_raw = rows[0].get('content') if rows else None
            try:
                cfg = json.loads(content_raw or '{}')
            except Exception:
                cfg = {}
            cfg.update(patch)
            sb.table('schedules').update({ 'content': json.dumps(cfg) }).eq('id', job['id']).execute()
        except Exception:
            pass
    return patch


@app.route('/', methods=['GET'])
@app.route('/cron_analyze', methods=['GET'])
@app.route('/api/cron_analyze', methods=['GET'])
def cron_analyze():
    try:
        sb = _load_sb()
        now = int(time.time() * 1000)
        # Schedules from supabase table 'schedules'
        # Support both numeric ms column 'runAt' and timestamptz 'run_at'
        from datetime import datetime, timezone
        iso_now = datetime.now(timezone.utc).isoformat()
        due = []
        try:
            r1 = sb.table('schedules').select('*').in_('status', ['pending','running']).lte('runAt', now).execute()
            due.extend(getattr(r1, 'data', []) or [])
        except Exception:
            pass
        try:
            r2 = sb.table('schedules').select('*').in_('status', ['pending','running']).lte('run_at', iso_now).execute()
            due.extend(getattr(r2, 'data', []) or [])
        except Exception:
            pass
        # schema v3 fallback: minimal table with (id, date, content)
        if not due:
            try:
                r3 = sb.table('schedules').select('id,content,created_at').execute()
                rows = getattr(r3, 'data', []) or []
                for row in rows:
                    try:
                        cfg = json.loads(row.get('content') or '{}')
                    except Exception:
                        cfg = {}
                    status = cfg.get('status', 'pending')
                    run_at = cfg.get('run_at')
                    if status in ('pending','running') and run_at:
                        row['status'] = status
                        row['run_at'] = run_at
                        row['scope'] = cfg.get('scope', 'all')
                        row['type'] = cfg.get('type', 'analysis')
                        row['remaining_ids'] = cfg.get('remaining_ids') or cfg.get('ids') or []
                        # ì‹œê°„ ì¡°ê±´
                        try:
                            ts = int(run_at)
                            if ts <= now:
                                due.append(row)
                        except Exception:
                            try:
                                if run_at <= iso_now:
                                    due.append(row)
                            except Exception:
                                pass
            except Exception:
                pass
        # de-duplicate by id if both queries returned rows
        seen = set(); unique = []
        for row in due:
            rid = str(row.get('id'))
            if rid in seen: continue
            seen.add(rid); unique.append(row)
        due = unique
        if not due:
            return jsonify({ 'ok': True, 'processed': 0 })

        processed = 0
        ranking_batch_size = int(os.getenv('RANKING_BATCH_SIZE', '250') or '250')
        analysis_batch_size = int(os.getenv('ANALYSIS_BATCH_SIZE', '3') or '3')
        time_budget_sec = int(os.getenv('RANKING_TIME_BUDGET', '40') or '40')
        for job in due:
            # take lease: set running
            try:
                sb.table('schedules').update({ 'status': 'running', 'updated_at': __import__('datetime').datetime.utcnow().isoformat() + 'Z' }).eq('id', job['id']).execute()
            except Exception:
                try:
                    cur = sb.table('schedules').select('content').eq('id', job['id']).limit(1).execute()
                    rows = getattr(cur, 'data', []) or []
                    content_raw = rows[0].get('content') if rows else None
                    try:
                        cfg = json.loads(content_raw or '{}')
                    except Exception:
                        cfg = {}
                    cfg.update({ 'status': 'running', 'updated_at': __import__('datetime').datetime.utcnow().isoformat() + 'Z' })
                    sb.table('schedules').update({ 'content': json.dumps(cfg) }).eq('id', job['id']).execute()
                except Exception:
                    pass
            if job.get('type') == 'ranking':
                deadline = time.time() + max(5, time_budget_sec)
                while time.time() < deadline:
                    patch = _process_job_batch(sb, job, batch_size=ranking_batch_size)
                    job['remainingIds'] = patch.get('remainingIds', job.get('remainingIds', []))
                    job['status'] = patch.get('status', job.get('status'))
                    if job['status'] == 'done' or not job.get('remainingIds'):
                        break
                # chain next job: analysis
                try:
                    if job.get('status') == 'done':
                        now_iso2 = __import__('datetime').datetime.utcnow().isoformat() + 'Z'
                        cfg = {
                            'type': 'analysis',
                            'scope': job.get('scope'),
                            'remaining_ids': job.get('remaining_ids') or job.get('ids') or [],
                            'status': 'pending',
                            'run_at': now_iso2,
                            'created_at': now_iso2,
                            'updated_at': now_iso2
                        }
                        sb.table('schedules').insert({ 'content': json.dumps(cfg), 'created_at': now_iso2 }).execute()
                except Exception:
                    pass
            else:
                _process_job_batch(sb, job, batch_size=analysis_batch_size)
            processed += 1
        return jsonify({ 'ok': True, 'processed': processed })
    except Exception as e:
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/health', methods=['GET'])
def health():
    return ('ok', 200)


@app.route('/analyze_one', methods=['POST'])
@app.route('/api/analyze_one', methods=['POST'])
def analyze_one():
    try:
        sb = _load_sb()
        body = {}
        try:
            body = request.get_json(force=True) or {}
        except Exception:
            body = {}
        vid = str(body.get('id') or request.args.get('id') or '').strip()
        if not vid:
            return jsonify({ 'ok': False, 'error': 'missing id' }), 400
        row = sb.table('videos').select('*').eq('id', vid).limit(1).execute()
        rows = getattr(row, 'data', []) or []
        if not rows:
            return jsonify({ 'ok': False, 'error': 'not_found' }), 404
        video = rows[0]
        updated = _analyze_video(video)
        if updated:
            sb.table('videos').update(updated).eq('id', vid).execute()
        return jsonify({ 'ok': True, 'updated': bool(updated) })
    except Exception as e:
        return jsonify({ 'ok': False, 'error': str(e) }), 500


